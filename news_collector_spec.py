#!/usr/bin/env python3
"""
ä»•æ§˜æ›¸å¯¾å¿œãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚·ã‚¹ãƒ†ãƒ 
Refinitiv EIKON APIã‹ã‚‰LMEéé‰„é‡‘å±ãƒ»å¸‚å ´ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
"""

import eikon as ek
import json
import logging
import time
import re
import pandas as pd
import asyncio
import warnings
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
from pathlib import Path

# pandas/numpy datetime64 å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã®è¨­å®š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
pd.set_option('mode.chained_assignment', None)

from models_spec import NewsArticle, SystemStats, extract_related_metals
from database_spec import SpecDatabaseManager
from gemini_analyzer import GeminiNewsAnalyzer

class RefinitivNewsCollector:
    """Refinitivãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å™¨ï¼ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰"""
    
    def __init__(self, config_path: str = "config_spec.json"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config = self._load_config(config_path)
        self.logger = self._setup_logger()
        self.db_manager = SpecDatabaseManager(self.config)
        
        # Geminiåˆ†æå™¨åˆæœŸåŒ–
        self.gemini_analyzer = GeminiNewsAnalyzer(self.config)
        
        # çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.stats = {
            'successful_queries': 0,
            'failed_queries': 0,
            'api_calls_made': 0,
            'errors_encountered': 0,
            'total_collected': 0,
            'ai_analyzed': 0,
            'ai_analysis_errors': 0
        }
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.existing_news_ids: Set[str] = set()
        
        # ã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶ç”¨ï¼ˆåŒã˜ã‚¨ãƒ©ãƒ¼ã®é‡è¤‡ã‚’é˜²ãï¼‰
        self.recent_errors: Dict[str, datetime] = {}
        self.error_cooldown_minutes = 5
        
        # EIKON APIåˆæœŸåŒ–
        try:
            ek.set_app_key(self.config["eikon_api_key"])
            self.logger.info("Refinitiv EIKON APIåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            self.logger.error(f"EIKON APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _load_config(self, config_path: str) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _safe_datetime_convert(self, dt_value) -> datetime:
        """å®‰å…¨ãªdatetimeå¤‰æ›ï¼ˆdatetime64ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰"""
        if dt_value is None:
            return datetime.now()
        
        try:
            # pandas Timestampã®å ´åˆ
            if hasattr(dt_value, 'to_pydatetime'):
                return dt_value.to_pydatetime()
            
            # numpy datetime64ã®å ´åˆï¼ˆunitæŒ‡å®šã§å®‰å…¨å¤‰æ›ï¼‰
            elif hasattr(dt_value, 'astype') and 'datetime64' in str(type(dt_value)):
                # datetime64ã‚’Timestampã«å¤‰æ›ã—ã¦ã‹ã‚‰datetimeã«
                ts = pd.Timestamp(dt_value)
                return ts.to_pydatetime()
            
            # æ–‡å­—åˆ—ã®å ´åˆ
            elif isinstance(dt_value, str):
                if 'T' in dt_value:
                    if dt_value.endswith('Z'):
                        return datetime.fromisoformat(dt_value.replace('Z', '+00:00'))
                    else:
                        return datetime.fromisoformat(dt_value.split('+')[0].split('Z')[0])
                else:
                    return pd.to_datetime(dt_value, errors='coerce').to_pydatetime()
            
            # ãã®ä»–ã®å‹ï¼ˆpandas to_datetimeã§å¤‰æ›ï¼‰
            else:
                result = pd.to_datetime(dt_value, errors='coerce', unit='ns')
                if pd.isna(result):
                    return datetime.now()
                return result.to_pydatetime()
                
        except Exception:
            # å…¨ã¦ã®å¤‰æ›ãŒå¤±æ•—ã—ãŸå ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’è¿”ã™
            return datetime.now()
    
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚°è¨­å®š"""
        logger = logging.getLogger('RefinitivNewsCollector')
        
        # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤ï¼ˆé‡è¤‡ã‚’é˜²ãï¼‰
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        logger.setLevel(getattr(logging, self.config["logging"]["log_level"]))
        
        # è¦ªãƒ­ã‚¬ãƒ¼ã‹ã‚‰ã®ä¼æ’­ã‚’é˜²ã
        logger.propagate = False
        
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        log_dir = Path(self.config["logging"]["log_directory"])
        log_dir.mkdir(exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®šï¼ˆå…¨ãƒ¬ãƒ™ãƒ«ï¼‰
        log_file = log_dir / f"refinitiv_news_{datetime.now().strftime('%Y%m%d')}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆWARNINGãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ã¿ - ã‚ˆã‚Šé™ã‹ï¼‰
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.WARNING)  # ERRORã‹ã‚‰WARNINGã«ç·©å’Œ
        console_formatter = logging.Formatter('%(levelname)s: %(message)s')  # ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        
        return logger
    
    def _load_existing_news_ids(self):
        """æ—¢å­˜ãƒ‹ãƒ¥ãƒ¼ã‚¹IDèª­ã¿è¾¼ã¿ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
        try:
            days_back = self.config["news_collection"]["duplicate_check_days"]
            self.existing_news_ids = set(self.db_manager.get_duplicate_news_ids(days_back))
            self.logger.info(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨IDèª­ã¿è¾¼ã¿å®Œäº†: {len(self.existing_news_ids)} ä»¶")
        except Exception as e:
            self.logger.warning(f"æ—¢å­˜IDèª­ã¿è¾¼ã¿è­¦å‘Š: {e}")
            self.existing_news_ids = set()
    
    def _clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not text:
            return ""
        
        # HTMLã‚¿ã‚°é™¤å»
        text = re.sub(r'<[^>]+>', '', text)
        
        # ä½™åˆ†ãªç©ºç™½é™¤å»
        text = re.sub(r'\s+', ' ', text).strip()
        
        # åˆ¶å¾¡æ–‡å­—é™¤å»
        text = ''.join(char for char in text if ord(char) >= 32 or char in '\n\t')
        
        return text
    
    def _extract_url_from_story(self, story_id: str) -> Optional[str]:
        """ã‚¹ãƒˆãƒ¼ãƒªãƒ¼IDã‹ã‚‰URLæŠ½å‡º"""
        try:
            # Refinitivã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´°å–å¾—
            story = ek.get_news_story(story_id)
            if story and hasattr(story, 'get'):
                # URLæƒ…å ±ãŒã‚ã‚Œã°å–å¾—
                url = story.get('url') or story.get('link')
                return url
        except Exception as e:
            self.logger.debug(f"URLå–å¾—ã‚¨ãƒ©ãƒ¼: {story_id} - {e}")
        
        return None
    
    def _get_news_by_query(self, query: str, start_date: datetime, end_date: datetime, collection_mode: str = "background") -> List[Dict]:
        """ã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾— - datetime64ã‚¨ãƒ©ãƒ¼å®Œå…¨å¯¾å¿œç‰ˆ"""
        try:
            self.logger.debug(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹: {query}")
            
            # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if not self._is_safe_query(query):
                self.logger.debug(f"å±é™ºãªã‚¯ã‚¨ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: {query}")
                return []
            
            # datetime64ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼šæ—¥ä»˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã‚ãšã«å–å¾—
            # EIKON APIã®æ—¥ä»˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒdatetime64ã‚¨ãƒ©ãƒ¼ã®æ ¹æœ¬åŸå› ã®ãŸã‚å®Œå…¨ã«å›é¿
            try:
                # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦é©åˆ‡ãªä»¶æ•°ã«è¨­å®šï¼ˆæ‰‹å‹•åé›†æ™‚ã¯å¤šã‚ã«å–å¾—ï¼‰
                max_per_query = self.config["news_collection"]["max_news_per_query"]
                if collection_mode == "manual":
                    # æ‰‹å‹•åé›†æ™‚ã¯æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹ãŸã‚å¤šã‚ã«
                    safe_count = min(max_per_query, 100)
                    self.logger.debug(f"æ‰‹å‹•åé›†ãƒ¢ãƒ¼ãƒ‰: {safe_count}ä»¶å–å¾—äºˆå®š")
                else:
                    safe_count = min(max_per_query, 50)
                    self.logger.debug(f"ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åé›†ãƒ¢ãƒ¼ãƒ‰: {safe_count}ä»¶å–å¾—äºˆå®š")
                
                # asyncioç«¶åˆå¯¾ç­–: EIKON APIå‘¼ã³å‡ºã—ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
                headlines = self._safe_eikon_call(query, safe_count)
                
                # ãƒ‡ãƒãƒƒã‚°: APIå–å¾—çµæœã®è©³ç´°ãƒ­ã‚°
                if headlines is not None and not headlines.empty:
                    self.logger.info(f"ğŸ“¡ APIå–å¾—çµæœ [{query}]: {len(headlines)}ä»¶")
                    
                    # æœ€æ–°ã¨æœ€å¤ã®è¨˜äº‹ã®æ—¥æ™‚ã‚’ç¢ºèª
                    if 'versionCreated' in headlines.columns:
                        dates = pd.to_datetime(headlines['versionCreated'], errors='coerce')
                        if not dates.empty:
                            latest = dates.max()
                            oldest = dates.min()
                            self.logger.info(f"  ğŸ“… è¨˜äº‹æ—¥æ™‚ç¯„å›²: {oldest} ï½ {latest}")
                    
                    # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚µãƒ³ãƒ—ãƒ«ï¼ˆä¸Šä½3ä»¶ï¼‰
                    for i, (idx, row) in enumerate(headlines.head(3).iterrows()):
                        title = str(row.get('text', ''))[:50]
                        created = row.get('versionCreated', 'N/A')
                        self.logger.info(f"  ğŸ“° [{i+1}] {created}: {title}...")
                else:
                    self.logger.warning(f"âŒ APIå–å¾—çµæœ [{query}]: 0ä»¶ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼")
                
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if headlines is not None and not headlines.empty:
                    headlines = self._filter_headlines_by_date(headlines, start_date, end_date)
                    self.logger.debug(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {query} - {len(headlines)} ä»¶")
                else:
                    headlines = pd.DataFrame()
                    
            except Exception as api_error:
                error_key = f"query_failed_{query}"
                if self._should_log_error(error_key):
                    self.logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å¤±æ•—: {query} (å…¨æ‰‹æ³•å¤±æ•—)")
                return []
            
            self.stats['api_calls_made'] += 1
            
            if headlines is None or headlines.empty:
                self.logger.debug(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {query}")
                return []
            
            news_items = []
            for idx, row in headlines.iterrows():
                try:
                    # åŸºæœ¬æƒ…å ±å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ã®åŸå› ã‚’ç‰¹å®šã™ã‚‹ãŸã‚æœ€å°é™ã«ï¼‰
                    story_id = str(row.get('storyId', ''))
                    headline = self._clean_text(str(row.get('text', '')))
                    source = str(row.get('sourceCode', ''))
                    
                    # æ—¢å­˜ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡é™¤å»ï¼‰
                    if story_id in self.existing_news_ids:
                        continue
                    
                    # æ—¥ä»˜å‡¦ç†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
                    publish_time = self._safe_datetime_convert(row.get('versionCreated'))
                    
                    # é™¤å¤–ã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
                    excluded_sources = self.config["news_collection"]["excluded_sources"]
                    if source.upper() in [s.upper() for s in excluded_sources]:
                        continue
                    
                    # æœ¬æ–‡å–å¾—
                    body = ""
                    url = None
                    if story_id:
                        try:
                            story = ek.get_news_story(story_id)
                            self.stats['api_calls_made'] += 1
                            
                            if story:
                                # è¾æ›¸å½¢å¼ã®å ´åˆ
                                if isinstance(story, dict):
                                    body = self._clean_text(story.get('storyHtml', '') or story.get('story', '') or story.get('text', ''))
                                    url = story.get('url') or story.get('link')
                                # æ–‡å­—åˆ—ã®å ´åˆ
                                elif isinstance(story, str):
                                    body = self._clean_text(story)
                                # ãã®ä»–ã®å½¢å¼
                                else:
                                    body = self._clean_text(str(story))
                                    
                            time.sleep(0.1)  # APIåˆ¶é™å¯¾ç­–
                        except Exception as e:
                            self.logger.debug(f"æœ¬æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {story_id} - {e}")
                            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æœ¬æ–‡ã¨ã—ã¦ä½¿ç”¨
                            body = headline
                    
                    # é–¢é€£é‡‘å±æŠ½å‡º
                    related_metals = extract_related_metals(headline, body)
                    
                    # LMEé–¢é€£ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if self.config["news_collection"]["lme_only_filter"]:
                        if not self._is_lme_related(headline, body, related_metals):
                            continue
                    
                    news_item = {
                        'story_id': story_id,
                        'headline': headline,
                        'body': body,
                        'source': source,
                        'publish_time': publish_time,
                        'url': url,
                        'related_metals': related_metals,
                        'query': query
                    }
                    
                    news_items.append(news_item)
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã«è¿½åŠ 
                    self.existing_news_ids.add(story_id)
                    
                except Exception as e:
                    self.logger.debug(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
                    continue
            
            # APIåˆ¶é™å¯¾ç­–
            time.sleep(self.config["news_collection"]["api_rate_limit_delay"])
            
            self._log_successful_query(query, len(news_items))
            self.stats['successful_queries'] += 1
            return news_items
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãƒ­ã‚°å‡ºåŠ›ã‚’åˆ¶å¾¡
            if not datetime64_retry_attempted:
                error_key = f"general_error_{query}_{type(e).__name__}"
                if self._should_log_error(error_key):
                    self.logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {query} - {e}")
            
            self.stats['failed_queries'] += 1
            self.stats['errors_encountered'] += 1
            return []
    
    def _is_lme_related(self, headline: str, body: str, related_metals: str) -> bool:
        """LMEé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã©ã†ã‹åˆ¤å®š"""
        text = f"{headline} {body}".lower()
        
        # LMEé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        lme_keywords = self.config["news_collection"]["lme_keywords"]
        for keyword in lme_keywords:
            if keyword.lower() in text:
                return True
        
        # é‡‘å±é–¢é€£ãƒã‚§ãƒƒã‚¯
        if related_metals:
            return True
        
        # å¸‚å ´é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        market_keywords = self.config["news_collection"]["market_keywords"]
        for keyword in market_keywords:
            if keyword.lower() in text:
                return True
        
        return False
    
    def _get_collection_period(self, collection_mode: str = "background") -> tuple[datetime, datetime]:
        """åé›†æœŸé–“è¨ˆç®—"""
        end_date = datetime.now()
        
        if collection_mode == "manual":
            # æ‰‹å‹•åé›†ã®å ´åˆã¯çŸ­ã„æœŸé–“ã‚’ä½¿ç”¨ï¼ˆæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç¢ºå®Ÿã«å–å¾—ï¼‰
            hours_back = self.config["news_collection"].get("manual_collection_period_hours", 2)
            self.logger.info(f"æ‰‹å‹•åé›†ãƒ¢ãƒ¼ãƒ‰: éå»{hours_back}æ™‚é–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼ˆæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹é‡è¦–ï¼‰")
        else:
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åé›†ã®å ´åˆã¯é€šå¸¸æœŸé–“
            hours_back = self.config["news_collection"]["collection_period_hours"]
            self.logger.info(f"ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åé›†ãƒ¢ãƒ¼ãƒ‰: éå»{hours_back}æ™‚é–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†")
        
        start_date = end_date - timedelta(hours=hours_back)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¿½åŠ 
        self.logger.info(f"ğŸ” åé›†æœŸé–“è©³ç´°:")
        self.logger.info(f"  é–‹å§‹æ™‚åˆ»: {start_date.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"  çµ‚äº†æ™‚åˆ»: {end_date.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"  æœŸé–“: {hours_back}æ™‚é–“")
        
        return start_date, end_date
    
    def _safe_eikon_call(self, query: str, count: int, max_retries: int = 3):
        """
        asyncioç«¶åˆå¯¾ç­–: å®‰å…¨ãªEIKON APIå‘¼ã³å‡ºã—
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            count: å–å¾—ä»¶æ•°
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        
        Returns:
            ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³DataFrame
        """
        import threading
        import queue
        import time
        
        for attempt in range(max_retries):
            try:
                # ã‚¹ãƒ¬ãƒƒãƒ‰é–“ã§ã®ãƒ‡ãƒ¼ã‚¿å—ã‘æ¸¡ã—ç”¨ã‚­ãƒ¥ãƒ¼
                result_queue = queue.Queue()
                exception_queue = queue.Queue()
                
                def eikon_worker():
                    """åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§EIKON APIå‘¼ã³å‡ºã—"""
                    try:
                        # æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã§asyncioã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã¨åˆ†é›¢
                        headlines = ek.get_news_headlines(
                            query=query,
                            count=count
                        )
                        result_queue.put(headlines)
                    except Exception as e:
                        exception_queue.put(e)
                
                # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
                worker_thread = threading.Thread(target=eikon_worker)
                worker_thread.daemon = True
                worker_thread.start()
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§çµæœå¾…æ©Ÿ
                worker_thread.join(timeout=30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if worker_thread.is_alive():
                    self.logger.warning(f"EIKON APIå‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {query} (è©¦è¡Œ {attempt + 1})")
                    continue
                
                # çµæœå–å¾—
                if not result_queue.empty():
                    headlines = result_queue.get()
                    self.logger.debug(f"EIKON APIå‘¼ã³å‡ºã—æˆåŠŸ: {query}")
                    return headlines
                elif not exception_queue.empty():
                    exception = exception_queue.get()
                    self.logger.warning(f"EIKON APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {query} - {exception} (è©¦è¡Œ {attempt + 1})")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                        continue
                    else:
                        raise exception
                else:
                    self.logger.warning(f"EIKON APIå‘¼ã³å‡ºã—çµæœä¸æ˜: {query} (è©¦è¡Œ {attempt + 1})")
                    
            except Exception as e:
                self.logger.error(f"EIKON APIå‘¼ã³å‡ºã—å¤±æ•—: {query} - {e} (è©¦è¡Œ {attempt + 1})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    return None
        
        return None

    def _is_safe_query(self, query: str) -> bool:
        """
        ã‚¯ã‚¨ãƒªãŒå®‰å…¨ï¼ˆdatetime64ã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã•ãªã„ï¼‰ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        """
        # å•é¡ŒãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³
        problematic_patterns = [
            "LME",
            "london metal exchange",
            "lme prices",
            "lme warehouse", 
            "lme stocks",
            "lme trading",
            "lme copper",
            "lme aluminium",
            "lme zinc",
            "lme lead", 
            "lme nickel",
            "lme tin",
            "metal production",
            "metal demand",
            "mining strike",
            "trade war metals"
        ]
        
        query_lower = query.lower()
        for pattern in problematic_patterns:
            if pattern.lower() in query_lower:
                return False
        
        return True
    
    def _optimize_query(self, query: str) -> Optional[str]:
        """
        ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–ãƒ»å•é¡Œã‚¯ã‚¨ãƒªã®ä»£æ›¿
        
        Args:
            query: å…ƒã®ã‚¯ã‚¨ãƒª
            
        Returns:
            æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã¾ãŸã¯Noneï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰
        """
        # æ—¢çŸ¥ã®å•é¡Œã‚¯ã‚¨ãƒªã¨ãã®ä»£æ›¿
        query_replacements = {
            'metal inventory': 'metals inventory',  # ã‚ˆã‚Šä¸€èˆ¬çš„ãªè¡¨ç¾
            'metal stockpile': 'metals stockpile',
            'metal market': 'metals market',
            'metal production': 'metals production'
        }
        
        # å•é¡Œã‚¯ã‚¨ãƒªã®ã‚¹ã‚­ãƒƒãƒ—ãƒªã‚¹ãƒˆ
        skip_queries = [
            # ç‰¹ã«å•é¡ŒãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã‚¯ã‚¨ãƒª
        ]
        
        query_lower = query.lower()
        
        # ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ãƒã‚§ãƒƒã‚¯
        if query_lower in [q.lower() for q in skip_queries]:
            return None
        
        # ä»£æ›¿ã‚¯ã‚¨ãƒªãƒã‚§ãƒƒã‚¯
        for original, replacement in query_replacements.items():
            if query_lower == original.lower():
                self.logger.debug(f"ã‚¯ã‚¨ãƒªæœ€é©åŒ–: '{query}' -> '{replacement}'")
                return replacement
        
        return query
    
    def _should_log_error(self, error_key: str) -> bool:
        """
        ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ã¹ãã‹ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡æŠ‘åˆ¶ï¼‰
        
        Args:
            error_key: ã‚¨ãƒ©ãƒ¼ã®è­˜åˆ¥ã‚­ãƒ¼
            
        Returns:
            ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã¹ãã‹ã©ã†ã‹
        """
        now = datetime.now()
        
        # éå»ã®ã‚¨ãƒ©ãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¤ã„ã‚‚ã®ã‚’å‰Šé™¤ï¼‰
        expired_keys = [
            key for key, timestamp in self.recent_errors.items()
            if (now - timestamp).total_seconds() > self.error_cooldown_minutes * 60
        ]
        for key in expired_keys:
            del self.recent_errors[key]
        
        # åŒã˜ã‚¨ãƒ©ãƒ¼ãŒæœ€è¿‘å‡ºåŠ›ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
        if error_key in self.recent_errors:
            time_since_last = (now - self.recent_errors[error_key]).total_seconds()
            if time_since_last < self.error_cooldown_minutes * 60:
                return False  # æœ€è¿‘åŒã˜ã‚¨ãƒ©ãƒ¼ãŒå‡ºåŠ›ã•ã‚Œã¦ã„ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
        
        # ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
        self.recent_errors[error_key] = now
        return True
    
    def _log_successful_query(self, query: str, count: int) -> None:
        """æˆåŠŸã—ãŸã‚¯ã‚¨ãƒªã‚’ãƒ­ã‚°è¨˜éŒ²"""
        if count > 0:
            self.logger.info(f"ã‚¯ã‚¨ãƒª '{query}' ã§ {count} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
        else:
            self.logger.debug(f"ã‚¯ã‚¨ãƒª '{query}' ã§ã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    def _filter_headlines_by_date(self, headlines: pd.DataFrame, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """
        ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æ—¥ä»˜ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ï¼‰
        
        Args:
            headlines: ç”Ÿã®ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³DataFrame
            start_date: é–‹å§‹æ—¥æ™‚
            end_date: çµ‚äº†æ—¥æ™‚
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸDataFrame
        """
        if headlines.empty:
            return headlines
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®çŠ¶æ³
        self.logger.info(f"ğŸ” æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹:")
        self.logger.info(f"  å¯¾è±¡æœŸé–“: {start_date.strftime('%Y-%m-%d %H:%M')} ï½ {end_date.strftime('%Y-%m-%d %H:%M')}")
        self.logger.info(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ä»¶æ•°: {len(headlines)}ä»¶")
        
        try:
            # æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
            date_column = None
            for col in headlines.columns:
                if 'versionCreated' in col or 'created' in col.lower() or 'date' in col.lower():
                    date_column = col
                    break
            
            if date_column is None:
                self.logger.warning("âŒ æ—¥ä»˜ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                self.logger.info(f"  åˆ©ç”¨å¯èƒ½ã‚«ãƒ©ãƒ : {list(headlines.columns)}")
                return headlines
            
            # æ—¥ä»˜ã‚’datetimeã«å¤‰æ›ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            filtered_headlines = []
            for idx, row in headlines.iterrows():
                try:
                    date_value = row[date_column]
                    publish_time = self._safe_datetime_convert(date_value)
                    
                    # æ—¥ä»˜ç¯„å›²ãƒã‚§ãƒƒã‚¯
                    if start_date <= publish_time <= end_date:
                        filtered_headlines.append(row)
                        
                except Exception as e:
                    self.logger.debug(f"æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å«ã‚ã‚‹
                    filtered_headlines.append(row)
            
            result_df = pd.DataFrame(filtered_headlines) if filtered_headlines else pd.DataFrame()
            
            # ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
            self.logger.info(f"âœ… æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†:")
            self.logger.info(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œä»¶æ•°: {len(result_df)}ä»¶")
            if len(result_df) == 0:
                self.logger.warning("âš ï¸  æŒ‡å®šæœŸé–“å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ - ã‚ˆã‚Šå¤šãã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹æœŸé–“ã‚’æ‹¡å¤§ã—ã¦ãã ã•ã„")
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ãŒå³ã—ã™ãã‚‹å ´åˆã®å¯¾ç­–ï¼šå°‘ãªãã¨ã‚‚æœ€æ–°ã®è¨˜äº‹ã‚’æ®‹ã™
                if not headlines.empty:
                    self.logger.info("ğŸ”„ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒå³ã—ã™ãã‚‹ãŸã‚ã€æœ€æ–°ã®è¨˜äº‹ã‚’æ®‹ã—ã¾ã™")
                    return headlines.head(10)  # æœ€æ–°10ä»¶ã‚’ä¿æŒ
            
            return result_df
                
        except Exception as e:
            self.logger.debug(f"æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            return headlines
    
    def collect_historical_news(self, months_back: int = 3) -> int:
        """éå»æ•°ãƒ¶æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€æ‹¬åé›†"""
        start_time = datetime.now()
        self.logger.info(f"éå»{months_back}ãƒ¶æœˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€æ‹¬åé›†é–‹å§‹")
        
        try:
            # åé›†æœŸé–“è¨ˆç®—ï¼ˆæ•°ãƒ¶æœˆï¼‰
            end_date = datetime.now()
            start_date = end_date - timedelta(days=months_back * 30)
            self.logger.info(f"åé›†æœŸé–“: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
            
            # æ—¢å­˜ãƒ‹ãƒ¥ãƒ¼ã‚¹IDèª­ã¿è¾¼ã¿
            self._load_existing_news_ids()
            
            all_news = []
            
            # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªã®ã¿ä½¿ç”¨ï¼ˆã‚¨ãƒ©ãƒ¼ãŒèµ·ãã«ãã„ã‚‚ã®ï¼‰
            basic_queries = [
                "copper",
                "aluminium", 
                "zinc",
                "lead",
                "nickel",
                "tin",
                "LME",
                "metals",
                "commodity",
                "mining"
            ]
            
            for query in basic_queries:
                self.logger.info(f"ã‚¯ã‚¨ãƒª '{query}' ã®åé›†é–‹å§‹")
                
                try:
                    # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°åˆ†ã‘ã—ã¦å–å¾—
                    current_start = start_date
                    while current_start < end_date:
                        current_end = min(current_start + timedelta(days=7), end_date)
                        
                        self.logger.info(f"æœŸé–“: {current_start.strftime('%Y-%m-%d')} - {current_end.strftime('%Y-%m-%d')}")
                        
                        try:
                            news_items = self._get_news_by_query(query, current_start, current_end)
                            all_news.extend(news_items)
                            self.logger.info(f"å–å¾—: {len(news_items)} ä»¶")
                            
                            # APIåˆ¶é™å¯¾ç­–
                            time.sleep(2)
                            
                        except Exception as e:
                            self.logger.warning(f"æœŸé–“åˆ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                        
                        current_start = current_end
                        
                    # ã‚¯ã‚¨ãƒªé–“ã®é–“éš”
                    time.sleep(3)
                    
                except Exception as e:
                    self.logger.error(f"ã‚¯ã‚¨ãƒª '{query}' ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            saved_count = 0
            if all_news:
                self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–‹å§‹: {len(all_news)} ä»¶")
                
                # é‡è¤‡é™¤å»
                unique_news = {}
                for item in all_news:
                    story_id = item.get('story_id', '')
                    if story_id and story_id not in unique_news:
                        unique_news[story_id] = item
                
                self.logger.info(f"é‡è¤‡é™¤å»å¾Œ: {len(unique_news)} ä»¶")
                
                # NewsArticleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                news_articles = []
                current_time = datetime.now()
                
                for item in unique_news.values():
                    try:
                        article = NewsArticle(
                            news_id=item['story_id'],
                            title=item['headline'],
                            body=item['body'],
                            publish_time=item['publish_time'],
                            acquire_time=current_time,
                            source=item['source'],
                            url=item.get('url'),
                            related_metals=item.get('related_metals'),
                            is_manual=False
                        )
                        news_articles.append(article)
                    except Exception as e:
                        self.logger.warning(f"è¨˜äº‹å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                # ãƒãƒƒãƒä¿å­˜
                saved_count = self.db_manager.insert_news_batch(news_articles)
                self.stats['total_collected'] = saved_count
            
            # çµ±è¨ˆä¿å­˜
            execution_time = (datetime.now() - start_time).total_seconds()
            
            self.logger.info(f"ä¸€æ‹¬åé›†å®Œäº†: {saved_count} ä»¶ä¿å­˜ã€å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
            return saved_count
            
        except Exception as e:
            self.logger.error(f"ä¸€æ‹¬åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def collect_news(self, collection_mode: str = "background") -> int:
        """
        ãƒ¡ã‚¤ãƒ³ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å‡¦ç†
        
        Args:
            collection_mode: "manual" or "background"
        """
        start_time = datetime.now()
        self.logger.info("Refinitivãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹")
        
        try:
            # æ—¢å­˜ãƒ‹ãƒ¥ãƒ¼ã‚¹IDèª­ã¿è¾¼ã¿
            self._load_existing_news_ids()
            
            # åé›†æœŸé–“è¨ˆç®—
            start_date, end_date = self._get_collection_period(collection_mode)
            self.logger.info(f"åé›†æœŸé–“: {start_date.strftime('%Y-%m-%d %H:%M')} - {end_date.strftime('%Y-%m-%d %H:%M')}")
            
            all_news = []
            
            # åé›†ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ã‚¯ã‚¨ãƒªã‚’é¸æŠ
            query_categories = self.config["news_collection"]["query_categories"]
            
            if collection_mode == "manual":
                # æ‰‹å‹•åé›†æ™‚ã¯å„ªå…ˆåº¦ã®é«˜ã„ã‚¯ã‚¨ãƒªã®ã¿ã‚’ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰
                priority_categories = ["lme_metals", "base_metals"]
                filtered_categories = {k: v for k, v in query_categories.items() if k in priority_categories}
                self.logger.info(f"æ‰‹å‹•åé›†ãƒ¢ãƒ¼ãƒ‰: å„ªå…ˆã‚«ãƒ†ã‚´ãƒªã®ã¿ä½¿ç”¨ ({', '.join(priority_categories)})")
            else:
                # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åé›†æ™‚ã¯å…¨ã‚«ãƒ†ã‚´ãƒªã‚’ä½¿ç”¨
                filtered_categories = query_categories
                self.logger.info("ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åé›†ãƒ¢ãƒ¼ãƒ‰: å…¨ã‚«ãƒ†ã‚´ãƒªä½¿ç”¨")
            
            for category, queries in filtered_categories.items():
                self.logger.info(f"ã‚«ãƒ†ã‚´ãƒª '{category}' ã®åé›†é–‹å§‹")
                
                category_failed_count = 0
                for query in queries:
                    # æ—¢çŸ¥ã®å•é¡Œã‚¯ã‚¨ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—ã¾ãŸã¯ä»£æ›¿
                    optimized_query = self._optimize_query(query)
                    if optimized_query is None:
                        self.logger.debug(f"å•é¡Œã‚¯ã‚¨ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: {query}")
                        continue
                    
                    try:
                        news_items = self._get_news_by_query(optimized_query, start_date, end_date, collection_mode)
                        all_news.extend(news_items)
                    except Exception:
                        category_failed_count += 1
                        # å€‹åˆ¥ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã¯_get_news_by_queryã§æŠ‘åˆ¶æ¸ˆã¿
                        continue
                    
                    # APIåˆ¶é™å¯¾ç­–ï¼ˆæ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯çŸ­ç¸®ï¼‰
                    if collection_mode == "manual":
                        # æ‰‹å‹•åé›†æ™‚ã¯çŸ­ã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«
                        time.sleep(self.config["news_collection"].get("query_interval", 1.0) * 0.3)
                    else:
                        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åé›†æ™‚ã¯é€šå¸¸ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«
                        time.sleep(self.config["news_collection"]["query_interval"])
                
                # ã‚«ãƒ†ã‚´ãƒªå˜ä½ã®ã‚µãƒãƒªãƒ¼ãƒ­ã‚°
                if category_failed_count > 0:
                    self.logger.debug(f"ã‚«ãƒ†ã‚´ãƒª '{category}': {category_failed_count}/{len(queries)} ã‚¯ã‚¨ãƒªå¤±æ•—")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã¨AIåˆ†æ
            saved_count = 0
            if all_news:
                news_articles = []
                current_time = datetime.now()
                
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
                for item in all_news:
                    article = NewsArticle(
                        news_id=item['story_id'],
                        title=item['headline'],
                        body=item['body'],
                        publish_time=item['publish_time'],
                        acquire_time=current_time,
                        source=item['source'],
                        url=item.get('url'),
                        related_metals=item.get('related_metals'),
                        is_manual=False
                    )
                    news_articles.append(article)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                saved_count = self.db_manager.insert_news_batch(news_articles)
                self.stats['total_collected'] = saved_count
                
                # AIåˆ†æå®Ÿè¡Œï¼ˆæ‰‹å‹•åé›†æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦é«˜é€ŸåŒ–ï¼‰
                if collection_mode == "background" and self.gemini_analyzer.gemini_config.get("enable_ai_analysis", False):
                    try:
                        self.logger.info(f"AIåˆ†æé–‹å§‹: {len(news_articles)} ä»¶")
                        asyncio.run(self._analyze_news_batch(news_articles))
                    except Exception as e:
                        self.logger.error(f"AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                        self.stats['ai_analysis_errors'] += 1
                elif collection_mode == "manual":
                    self.logger.info("æ‰‹å‹•åé›†ãƒ¢ãƒ¼ãƒ‰: AIåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰")
            
            # çµ±è¨ˆä¿å­˜
            execution_time = (datetime.now() - start_time).total_seconds()
            stats = SystemStats(
                collection_date=datetime.now(),
                total_collected=saved_count,
                successful_queries=self.stats['successful_queries'],
                failed_queries=self.stats['failed_queries'],
                api_calls_made=self.stats['api_calls_made'],
                errors_encountered=self.stats['errors_encountered'],
                execution_time_seconds=execution_time
            )
            
            self.db_manager.insert_system_stats(stats)
            
            # çµ±è¨ˆã‚µãƒãƒªãƒ¼è¡¨ç¤º
            total_queries = self.stats['successful_queries'] + self.stats['failed_queries']
            success_rate = (self.stats['successful_queries'] / total_queries * 100) if total_queries > 0 else 0
            
            # é‡è¦ãªå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚è¡¨ç¤ºï¼ˆWARNINGä»¥ä¸Šï¼‰
            if saved_count > 0:
                self.logger.warning(f"âœ“ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†: {saved_count} ä»¶ä¿å­˜ã€å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
            else:
                self.logger.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†: {saved_count} ä»¶ä¿å­˜ã€å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
            
            self.logger.info(f"çµ±è¨ˆ: æˆåŠŸã‚¯ã‚¨ãƒª {self.stats['successful_queries']}/{total_queries} ({success_rate:.1f}%), APIå‘¼ã³å‡ºã— {self.stats['api_calls_made']} å›")
            
            # ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„å ´åˆã®ã¿è­¦å‘Š
            if total_queries > 0 and success_rate < 70:
                self.logger.warning(f"ã‚¯ã‚¨ãƒªæˆåŠŸç‡ãŒä½ä¸‹ã—ã¦ã„ã¾ã™ ({success_rate:.1f}%) - APIçŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            
            # ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼è¡¨ç¤ºï¼ˆå€‹åˆ¥ã‚¨ãƒ©ãƒ¼ã®ä»£ã‚ã‚Šï¼‰
            if self.stats['failed_queries'] > 0:
                self.logger.warning(f"ä¸€éƒ¨ã‚¯ã‚¨ãƒªãŒå¤±æ•—ã—ã¾ã—ãŸ: {self.stats['failed_queries']} ä»¶ï¼ˆè©³ç´°ã¯ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªï¼‰")
            
            return saved_count
            
        except Exception as e:
            self.logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats['errors_encountered'] += 1
            return 0
    
    async def _analyze_news_batch(self, news_articles: List[NewsArticle]):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€æ‹¬AIåˆ†æ"""
        try:
            # NewsArticleã‚’Dictå½¢å¼ã«å¤‰æ›
            news_dicts = []
            for article in news_articles:
                news_dict = {
                    'news_id': article.news_id,
                    'title': article.title,
                    'body': article.body,
                    'source': article.source,
                    'publish_time': article.publish_time,
                    'related_metals': article.related_metals
                }
                news_dicts.append(news_dict)
            
            # AIåˆ†æå®Ÿè¡Œ
            analysis_results = await self.gemini_analyzer.analyze_news_batch(news_dicts)
            
            # åˆ†æçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›´æ–°
            for news_id, result in analysis_results:
                try:
                    self.db_manager.update_news_analysis(news_id, {
                        'summary': result.summary,
                        'sentiment': result.sentiment,
                        'keywords': result.keywords,
                        'importance_score': result.importance_score
                    })
                    self.stats['ai_analyzed'] += 1
                except Exception as e:
                    self.logger.error(f"åˆ†æçµæœæ›´æ–°ã‚¨ãƒ©ãƒ¼ {news_id}: {e}")
            
            self.logger.info(f"AIåˆ†æå®Œäº†: {len(analysis_results)} ä»¶")
            
        except Exception as e:
            self.logger.error(f"AIåˆ†æãƒãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")
            self.stats['ai_analysis_errors'] += 1
    
    def get_collection_status(self) -> Dict:
        """åé›†çŠ¶æ³å–å¾—"""
        base_stats = {
            'successful_queries': self.stats['successful_queries'],
            'failed_queries': self.stats['failed_queries'],
            'api_calls_made': self.stats['api_calls_made'],
            'errors_encountered': self.stats['errors_encountered'],
            'total_collected': self.stats['total_collected'],
            'existing_news_count': len(self.existing_news_ids),
            'ai_analyzed': self.stats['ai_analyzed'],
            'ai_analysis_errors': self.stats['ai_analysis_errors']
        }
        
        # Geminiåˆ†æçµ±è¨ˆã‚’è¿½åŠ 
        if hasattr(self, 'gemini_analyzer') and self.gemini_analyzer:
            try:
                gemini_stats = self.gemini_analyzer.get_analysis_stats()
                base_stats['gemini_analysis'] = gemini_stats
            except Exception as e:
                self.logger.error(f"Geminiçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                base_stats['gemini_analysis'] = {'error': str(e)}
        
        return base_stats

class NewsPollingService:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒ¼ãƒªãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, config_path: str = "config_spec.json"):
        """åˆæœŸåŒ–"""
        self.collector = RefinitivNewsCollector(config_path)
        self.config = self.collector.config
        self.logger = self.collector.logger
        self.is_running = False
    
    def start_polling(self):
        """ãƒãƒ¼ãƒªãƒ³ã‚°é–‹å§‹"""
        self.is_running = True
        self.logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒ¼ãƒªãƒ³ã‚°é–‹å§‹")
        
        while self.is_running:
            try:
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Ÿè¡Œ
                collected_count = self.collector.collect_news()
                self.logger.info(f"ãƒãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œå®Œäº†: {collected_count} ä»¶åé›†")
                
                # æ¬¡å›å®Ÿè¡Œã¾ã§å¾…æ©Ÿ
                polling_interval = self.config["news_collection"]["polling_interval_minutes"]
                self.logger.info(f"æ¬¡å›å®Ÿè¡Œã¾ã§ {polling_interval} åˆ†å¾…æ©Ÿ")
                
                for i in range(polling_interval * 60):  # ç§’å˜ä½ã§å¾…æ©Ÿ
                    if not self.is_running:
                        break
                    time.sleep(1)
                    
            except KeyboardInterrupt:
                self.logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
                break
            except Exception as e:
                self.logger.error(f"ãƒãƒ¼ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(60)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1åˆ†å¾…æ©Ÿ
        
        self.logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒãƒ¼ãƒªãƒ³ã‚°çµ‚äº†")
    
    def stop_polling(self):
        """ãƒãƒ¼ãƒªãƒ³ã‚°åœæ­¢"""
        self.is_running = False
        self.logger.info("ãƒãƒ¼ãƒªãƒ³ã‚°åœæ­¢è¦æ±‚")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        collector = RefinitivNewsCollector()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ
        if not collector.db_manager.test_connection():
            print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Ÿè¡Œ
        saved_count = collector.collect_news()
        print(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†: {saved_count} ä»¶ä¿å­˜")
        
        # åé›†çŠ¶æ³è¡¨ç¤º
        status = collector.get_collection_status()
        print(f"å®Ÿè¡Œçµ±è¨ˆ: {status}")
        
    except Exception as e:
        print(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()